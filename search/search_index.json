{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HSI-Remote_Sensing","text":"<p>Welcome to the documentation for HSI-Remote_Sensing, a collection of subspace-based clustering algorithms for hyperspectral datasets.</p> <p>This project provides implementations in Python, Julia, and MATLAB, with a strong focus on remote sensing datasets such as:</p> <ul> <li>Pavia University Scene  </li> <li>Salinas Scene  </li> <li>ONERA Satellite Change Detection  </li> </ul> <p>Use the navigation menu to explore algorithm details and usage examples.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This page walks you through setting up the repository and running your first clustering experiment.</p>"},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/Cristy210/HSI-Remote_Sensing.git\ncd HSI-Remote_Sensing\n</code></pre>"},{"location":"getting-started/#2-install-dependencies-python","title":"2. Install dependencies (Python)","text":"<p>This project uses a Conda environment for reproducibility and to support MLX (Apple Silicon GPU acceleration). Create and activate the environment</p> <pre><code>conda env create -f env.yml\nconda activate HSIRS\n</code></pre>"},{"location":"getting-started/#3-run-a-test-sample-kss-python","title":"3. Run a Test Sample (KSS Python)","text":"<pre><code>from KSS.kss_mlx import KSS\nimport numpy as np\n\n# Create random data matrix (D x N)\nX = np.random.randn(100, 500)\n\nkss = KSS(K=3, r=2, max_iter=10)\nlabels = kss.fit_predict(X)\n\nprint(labels)\n</code></pre>"},{"location":"matlab-kss/","title":"MATLAB K-Subspaces (KSS)","text":"<p>The MATLAB implementation of the K-Subspaces algorithm is provided in the <code>KSS_MATLAB</code> folder.</p>"},{"location":"matlab-kss/#files","title":"Files","text":"<ul> <li><code>kss.m</code> \u2013 core KSS algorithm (D \u00d7 N data matrix, K clusters, r-dimensional subspaces)</li> <li><code>kss_rs.m</code> \u2013 example script for running KSS on the Pavia dataset</li> </ul>"},{"location":"matlab-kss/#example-usage","title":"Example usage","text":"<pre><code>addpath(\"KSS_MATLAB\");\n\nload(\"MAT Files/Pavia.mat\");\nload(\"GT Files/Pavia_gt.mat\");\n\n% X: D x N data matrix constructed from the cube\n[U, c, total_cost] = kss(X, K, r);\n\n% See kss_rs.m for a complete example on Pavia\n</code></pre>"},{"location":"python-kss/","title":"Python K-Subspaces (KSS)","text":"<p>This page documents the Python implementation of the K-Subspaces (KSS) algorithm. The main implementation lives in <code>KSS/kss_mlx.py</code> using MLX framework for Apple Silicon acceleration.</p>"},{"location":"python-kss/#kss-class","title":"KSS class","text":"<p>K-Subspaces (KSS) Clusering</p>"},{"location":"python-kss/#KSS.kss_mlx.KSS--parameters","title":"Parameters","text":"<p>n_clusters: int     Number of clusters. subspaces_dims = int or sequence of int     Dimension(s) of each subspace. If an int is provided, all subspaces     will have the same dimension. max_iter = int, default=100     Maximum number of iterations. n_int = int, default=10     Number of random initializations. The best run (Highest cost) is kept verbose = int, default=0     Verbosity level. 0=Silent, 1=per-run messsages, 2=per-iter bar. random_state = int, default=None     Random seed for reproducibility.</p> Source code in <code>KSS/kss_mlx.py</code> <pre><code>class KSS:\n    \"\"\"\n    K-Subspaces (KSS) Clusering\n\n    Parameters\n    ----------\n    n_clusters: int\n        Number of clusters.\n    subspaces_dims = int or sequence of int\n        Dimension(s) of each subspace. If an int is provided, all subspaces\n        will have the same dimension.\n    max_iter = int, default=100\n        Maximum number of iterations.\n    n_int = int, default=10\n        Number of random initializations. The best run (Highest cost) is kept\n    verbose = int, default=0\n        Verbosity level. 0=Silent, 1=per-run messsages, 2=per-iter bar.\n    random_state = int, default=None\n        Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_clusters:int,\n        subspaces_dims:Union[int, Iterable[int]],\n        max_iter:int=100,\n        n_init:int=10,\n        verbose:int=0,\n        random_state:Optional[int]=None,\n    ) -&gt; None:\n\n        self.n_clusters = int(n_clusters)\n        self.subspaces_dims = subspaces_dims\n        self.max_iter = int(max_iter)\n        self.n_init = int(n_init)\n        self.verbose = int(verbose)\n        self.random_state = random_state\n\n        # attributes filled by fit()\n        self.labels_:Optional[np.ndarray] = None\n        self.subspaces_:Optional[List[Array]] = None\n        self.cost_:Optional[float] = None\n\n    def _check_susbpace_dims(self) -&gt; List[int]:\n        if isinstance(self.subspaces_dims, int):\n            d = [int(self.subspaces_dims)] * self.n_clusters\n            return d\n        else:\n            d = [int(di) for di in self.subspaces_dims]\n            if len(d) != self.n_clusters:\n                raise ValueError(\n                    \"Length of subspaces_dims must match n_clusters\"\n                    f\"({len(d)} != {self.n_clusters}).\"\n                )\n\n        return d\n\n    @staticmethod\n    def _to_mlx_column_major(X) -&gt; Tuple[Array, int, int]:\n        \"\"\"\n        Convert input X to MLX array:\n\n        Accepts:\n            - Numpy Array of shape(n_samples, n_features)\n            - MLX Array of shape (n_features, n_samples)\n        \"\"\"\n\n        if isinstance(X, mx.array):\n            D, N = X.shape\n            return X, D, N\n\n        X_np = np.asarray(X, dtype=np.float64)\n        if X_np.ndim != 2:\n            raise ValueError(\"Input data X must be 2D (n_samples, n_features).\")\n\n        n_samples, n_features = X_np.shape\n        X_mlx = mx.array(X_np.T, dtype=mx.float64)\n        return X_mlx, n_features, n_samples\n\n    @staticmethod\n    def _cost(U: Sequence[Array], X:Array, labels: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute cost: sum over i of ||U_k^T x_i||^2 for assigned cluster k.\n        \"\"\"\n\n        K = len(U)\n        scores = mx.stack(\n            [mx.sum(mx.matmul(Uk.T, X, stream=mx.gpu) ** 2, axis=0) for Uk in U],\n            axis=0,\n        )\n        scores_np = np.array(scores)\n        c0 = labels\n        return float(scores_np[c0, np.arange(scores_np.shape[1])].sum())\n\n    def _kss_single(\n        self,\n        X: Array,\n        d: Sequence[int],\n        seed: Optional[int] = None,\n    ) -&gt; Tuple[List[Array], np.ndarray, float]:\n        \"\"\"\n        Single run of KSS clustering.\n\n        Parameters\n        ----------\n        X : (D, N) mx.array\n            Data matrix with N points in R^D (columns are points).\n        d : sequence of int, length K\n            Subspace dimensions for each cluster.\n        seed : int, optional\n            Random seed for initialization.\n\n        Returns\n        -------\n        U : list of (D, d_k) mx.array\n            Learned subspace bases.\n        c: (N,) np.ndarray of ints in 0...K-1\n            Cluster labels.\n        cost: float\n            Cost of each run.\n        \"\"\"\n        if seed is not None:\n            mx.random.seed(seed)\n\n        K = len(d)\n        D, N = X.shape\n\n        # Initialize subspaces\n        U: List[Array] = [\n            polar(mx.random.normal(shape=(D, dk)))\n            for dk in d\n        ]\n\n        # Initial cluster assignment\n        scores = mx.stack(\n            [mx.sum(mx.matmul(Uk.T, X, stream=mx.gpu) ** 2, axis=0) for Uk in U],\n            axis=0,\n        )\n        labels = np.argmax(np.array(scores), axis=0).astype(np.int32)\n        labels_prev = labels.copy()\n\n        # Iterations\n        if self.verbose &gt;= 2:\n            iter_range = trange(self.max_iter, desc=\"KSS\", leave=False)\n        else:\n            iter_range = range(self.max_iter)\n\n        for t in iter_range:\n            # Update subspaces\n            for k in range(K):\n                ilist = np.nonzero(labels == k)[0]\n\n                if ilist.size == 0:\n                    # Empty cluster: reinitialize its subspace\n                    U[k] = polar(mx.random.normal(shape=(D, d[k])))\n                    continue\n                idx = mx.array(ilist)\n                X_k = mx.take(X, idx, axis=1)\n                A = mx.matmul(X_k, X_k.T, stream=mx.gpu)\n                w, V = mx.linalg.eigh(A, stream=mx.cpu)\n                U[k] = V[:, -d[k]:]\n\n            # Update clusters\n            scores = mx.stack(\n                [mx.sum(mx.matmul(Uk.T, X, stream=mx.gpu) ** 2, axis=0) for Uk in U],\n                axis=0,\n            )\n            labels = np.argmax(np.array(scores), axis=0).astype(np.int32)\n\n            # Break if clusters did not change, update otherwise\n            if np.array_equal(labels, labels_prev):\n                if self.verbose &gt;= 2:\n                    print(f\"KSS terminated early at iteration {t + 1}\")\n                break\n            labels_prev = labels.copy()\n\n        # Compute final cost\n        cost = self._cost(U, X, labels)\n\n        return U, labels, cost\n\n    # Public API\n    def fit(self, X) -&gt; \"KSS\":\n        \"\"\"\n        Compute K-Subspaces clustering.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or MLX array (D, N)\n        Returns\n        -------\n        self : KSS\n            Fitted estimator.\n        \"\"\"\n\n        X_mx, D, N = self._to_mlx_column_major(X)\n        d = self._check_susbpace_dims()\n\n        if self.verbose &gt;= 1:\n            print(\n                f\"Running KSS with n_clusters={self.n_clusters}\"\n                f\", subspaces_dims={d}, max_iter={self.max_iter}, n_init={self.n_init}\"\n            )\n\n        best_cost = -np.inf\n        best_labels = None\n        best_U: Optional[List[Array]] = None\n\n        for run in range(self.n_init):\n            if self.verbose &gt;= 1:\n                print(f\" KSS run {run + 1}/{self.n_init}\")\n\n            seed = None if self.random_state is None else self.random_state + run\n            U_run, labels_run, cost_run = self._kss_single(X_mx, d, seed=seed)\n\n            if self.verbose &gt;= 1:\n                print(f\"  Run cost: {cost_run:.4e}\")\n\n            if cost_run &gt; best_cost:\n                best_cost = cost_run\n                best_labels = labels_run.copy()\n                best_U = [Uk for Uk in U_run]\n\n        assert best_U is not None and best_labels is not None\n        self.subspaces_ = best_U\n        self.labels_ = best_labels.astype(np.int32) + 1\n        self.cost_ = best_cost\n\n        return self\n\n    def fit_predict(self, X, y=None) -&gt; np.ndarray:\n        \"\"\"\n        Fit KSS and return labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or MLX array (D, N)\n\n        Returns\n        -------\n        labels: (n_samples, )\n            Labels in {1, ..., n_clusters}.\n        \"\"\"\n        self.fit(X, y=y)\n        return self.labels_\n\n    def predict(self, X) -&gt; np.ndarray:\n        \"\"\"\n        Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or MLX array (D, N)\n\n        Returns\n        -------\n        labels: (n_samples, )\n            Labels in {1, ..., n_clusters}.\n        \"\"\"\n\n        if self.subspaces_ is None:\n            raise ValueError(\"KSS instance is not fitted yet. Call 'fit' first.\")\n\n        X_mx, D, N = self._to_mlx_column_major(X)\n        K = len(self.subspaces_)\n        U = self.subspaces_\n\n        scores = mx.stack(\n            [\n                mx.sum(mx.matmul(Uk.T, X_mx, stream=mx.gpu) ** 2, axis=0)\n                for Uk in U\n            ],\n            axis=0,\n        )\n        labels = np.argmax(np.array(scores), axis=0).astype(np.int32) + 1\n\n        return labels\n</code></pre>"},{"location":"python-kss/#KSS.kss_mlx.KSS.fit","title":"<code>fit(X)</code>","text":"<p>Compute K-Subspaces clustering. Parameters</p> <p>X : array-like of shape (n_samples, n_features) or MLX array (D, N) Returns</p> <p>self : KSS     Fitted estimator.</p> Source code in <code>KSS/kss_mlx.py</code> <pre><code>def fit(self, X) -&gt; \"KSS\":\n    \"\"\"\n    Compute K-Subspaces clustering.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or MLX array (D, N)\n    Returns\n    -------\n    self : KSS\n        Fitted estimator.\n    \"\"\"\n\n    X_mx, D, N = self._to_mlx_column_major(X)\n    d = self._check_susbpace_dims()\n\n    if self.verbose &gt;= 1:\n        print(\n            f\"Running KSS with n_clusters={self.n_clusters}\"\n            f\", subspaces_dims={d}, max_iter={self.max_iter}, n_init={self.n_init}\"\n        )\n\n    best_cost = -np.inf\n    best_labels = None\n    best_U: Optional[List[Array]] = None\n\n    for run in range(self.n_init):\n        if self.verbose &gt;= 1:\n            print(f\" KSS run {run + 1}/{self.n_init}\")\n\n        seed = None if self.random_state is None else self.random_state + run\n        U_run, labels_run, cost_run = self._kss_single(X_mx, d, seed=seed)\n\n        if self.verbose &gt;= 1:\n            print(f\"  Run cost: {cost_run:.4e}\")\n\n        if cost_run &gt; best_cost:\n            best_cost = cost_run\n            best_labels = labels_run.copy()\n            best_U = [Uk for Uk in U_run]\n\n    assert best_U is not None and best_labels is not None\n    self.subspaces_ = best_U\n    self.labels_ = best_labels.astype(np.int32) + 1\n    self.cost_ = best_cost\n\n    return self\n</code></pre>"},{"location":"python-kss/#KSS.kss_mlx.KSS.fit_predict","title":"<code>fit_predict(X, y=None)</code>","text":"<p>Fit KSS and return labels.</p>"},{"location":"python-kss/#KSS.kss_mlx.KSS.fit_predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features) or MLX array (D, N)</p>"},{"location":"python-kss/#KSS.kss_mlx.KSS.fit_predict--returns","title":"Returns","text":"<p>labels: (n_samples, )     Labels in {1, ..., n_clusters}.</p> Source code in <code>KSS/kss_mlx.py</code> <pre><code>def fit_predict(self, X, y=None) -&gt; np.ndarray:\n    \"\"\"\n    Fit KSS and return labels.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or MLX array (D, N)\n\n    Returns\n    -------\n    labels: (n_samples, )\n        Labels in {1, ..., n_clusters}.\n    \"\"\"\n    self.fit(X, y=y)\n    return self.labels_\n</code></pre>"},{"location":"python-kss/#KSS.kss_mlx.KSS.predict","title":"<code>predict(X)</code>","text":"<p>Predict the closest cluster each sample in X belongs to.</p>"},{"location":"python-kss/#KSS.kss_mlx.KSS.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features) or MLX array (D, N)</p>"},{"location":"python-kss/#KSS.kss_mlx.KSS.predict--returns","title":"Returns","text":"<p>labels: (n_samples, )     Labels in {1, ..., n_clusters}.</p> Source code in <code>KSS/kss_mlx.py</code> <pre><code>def predict(self, X) -&gt; np.ndarray:\n    \"\"\"\n    Predict the closest cluster each sample in X belongs to.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or MLX array (D, N)\n\n    Returns\n    -------\n    labels: (n_samples, )\n        Labels in {1, ..., n_clusters}.\n    \"\"\"\n\n    if self.subspaces_ is None:\n        raise ValueError(\"KSS instance is not fitted yet. Call 'fit' first.\")\n\n    X_mx, D, N = self._to_mlx_column_major(X)\n    K = len(self.subspaces_)\n    U = self.subspaces_\n\n    scores = mx.stack(\n        [\n            mx.sum(mx.matmul(Uk.T, X_mx, stream=mx.gpu) ** 2, axis=0)\n            for Uk in U\n        ],\n        axis=0,\n    )\n    labels = np.argmax(np.array(scores), axis=0).astype(np.int32) + 1\n\n    return labels\n</code></pre>"}]}